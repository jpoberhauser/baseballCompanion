{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940c2053",
   "metadata": {},
   "source": [
    "## Downloading and Transcribing a Youtube Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac08009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import yt_dlp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eba89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio(youtube_url, out_dir=\"downloads\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': f'{out_dir}/%(id)s.%(ext)s',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "        }]\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=True)\n",
    "        return f\"{out_dir}/{info['id']}.mp3\", info['id']\n",
    "    \n",
    "def transcribe(audio_path, model_size=\"medium\", output_dir=\"transcripts\"):\n",
    "    model = WhisperModel(model_size, compute_type=\"int8\")\n",
    "    segments, _ = model.transcribe(audio_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    text_chunks = []\n",
    "    for seg in segments:\n",
    "        text_chunks.append({\n",
    "            \"start\": seg.start,\n",
    "            \"end\": seg.end,\n",
    "            \"text\": seg.text\n",
    "        })\n",
    "\n",
    "    output_path = os.path.join(output_dir, os.path.basename(audio_path).replace(\".mp3\", \".txt\"))\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for chunk in text_chunks:\n",
    "            f.write(f\"{chunk['text']}\\n\")\n",
    "\n",
    "    print(f\"Transcription saved to: {output_path}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a56df9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=EKkFOMzwMgc\n",
      "[youtube] EKkFOMzwMgc: Downloading webpage\n",
      "[youtube] EKkFOMzwMgc: Downloading tv client config\n",
      "[youtube] EKkFOMzwMgc: Downloading tv player API JSON\n",
      "[youtube] EKkFOMzwMgc: Downloading ios player API JSON\n",
      "[youtube] EKkFOMzwMgc: Downloading m3u8 information\n",
      "[info] EKkFOMzwMgc: Downloading 1 format(s): 251\n",
      "[download] Destination: /Users/jpoberhauser/Desktop/baseballCompanion/data//EKkFOMzwMgc.webm\n",
      "[download] 100% of   50.77MiB in 00:00:01 at 33.38MiB/s    \n",
      "[ExtractAudio] Destination: /Users/jpoberhauser/Desktop/baseballCompanion/data//EKkFOMzwMgc.mp3\n",
      "Deleting original file /Users/jpoberhauser/Desktop/baseballCompanion/data//EKkFOMzwMgc.webm (pass -k to keep)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/jpoberhauser/Desktop/baseballCompanion/data//EKkFOMzwMgc.mp3',\n",
       " 'EKkFOMzwMgc')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_audio('https://www.youtube.com/watch?v=EKkFOMzwMgc', out_dir = '/Users/jpoberhauser/Desktop/baseballCompanion/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d453ec9",
   "metadata": {},
   "source": [
    "This is example is a 24 minute youtube clip and it gets compeltey transcribed in ~13 minutes using the medium model and in using the small model ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f2e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to: /Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/EKkFOMzwMgc.txt\n"
     ]
    }
   ],
   "source": [
    "transcribe('/Users/jpoberhauser/Desktop/baseballCompanion/data/EKkFOMzwMgc.mp3',\n",
    "            model_size=\"small\", \n",
    "            output_dir=\"/Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14bc06",
   "metadata": {},
   "source": [
    "### Let's generate and store embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "811a5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad5c1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers faiss-cpu\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45675c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01919569,  0.12008531,  0.15959838, ..., -0.00536285,\n",
       "        -0.08109499,  0.05021335],\n",
       "       [-0.01869035,  0.04151865,  0.0743155 , ...,  0.00486595,\n",
       "        -0.06190439,  0.0318751 ],\n",
       "       [ 0.13650198,  0.08227322, -0.02526161, ...,  0.08762044,\n",
       "         0.03045845, -0.01075751]], shape=(3, 384), dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8895bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6660, 0.1046],\n",
      "        [0.6660, 1.0000, 0.1411],\n",
      "        [0.1046, 0.1411, 1.0000]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff1cb23",
   "metadata": {},
   "source": [
    "## compare model.similarity to FAISS\n",
    "\n",
    "* obviously its oerkill to use FAISS for three sentence embeddings, but we will need it for hundreds of thousands of text chunks in a real vectorDB. \n",
    "\n",
    "* just to make sure we get similar results, we run the code below, and indeed, the first sentence is most similar to the second one, and they both give a similairyt of _around_ 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06d04f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [[0 1 2]]\n",
      "Distances: [[0.         0.66808915 1.7908318 ]]\n"
     ]
    }
   ],
   "source": [
    "#### compare to FAISS\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 = Euclidean, or use IndexFlatIP for cosine\n",
    "index.add(embeddings)\n",
    "\n",
    "# Query similarity from emb1\n",
    "D, I = index.search(embeddings[0].reshape(1, -1), k=3)\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", I)\n",
    "print(\"Distances:\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0de15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/CFtDihdNwJk.txt',\n",
       " '/Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/b5-xIieufYs.txt',\n",
       " '/Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/EKkFOMzwMgc.txt']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from langchain.schema import Document\n",
    "all_transcripts = glob.glob('/Users/jpoberhauser/Desktop/baseballCompanion/data/transcripts/*.txt')\n",
    "all_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "75acb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_docs = []\n",
    "for i in all_transcripts:\n",
    "    with open(i, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()  # `text` is one big string\n",
    "    all_raw_docs.append([text.replace('\\n', '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "303cb2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae0bfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(inner[0]) for inner in all_raw_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc7eb2",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1a76e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import  util\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f26ca046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "748261ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = [x.page_content for x in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28bf4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0bd5c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: The New York Mets are very good\n",
      "Most similar: York Mets, they went and got Juan Soto. They went and signed Francis Golden Door back. They've been bringing players in. Having the richest owner in the sport obviously helps that. If you go down the line, the Cubs made a big play for Tucker and the rest of the Astros team. They bring him over. Now, they're performing very well. The Dodgers have went out and signed everybody. The Padres and all the trades they've made for Michael King, Dylan Cease, you know, they've done a lot. Champions League of Giants have landed a bunch of free agents. That's been the difference. They've just got the good players when the good players were available. If you look at the American League team, name me a team that's done that. Yeah. There isn't a team that's went out and got the big player. The Yankees, they did that and they made it to the World Series. They went out and got Juan Soto. Then this year, you know, they went out and made some trades for Belly for Williams. Those haven't been great, but\n",
      "Scores: tensor([0.4128, 0.4564, 0.3222, 0.1766, 0.2548, 0.4010, 0.1918, 0.1514, 0.1744,\n",
      "        0.1575, 0.3642, 0.2610, 0.2716, 0.1940, 0.2822, 0.4498, 0.2874, 0.3609,\n",
      "        0.4467, 0.3794, 0.2364, 0.3713, 0.3025, 0.4623, 0.4031, 0.3182, 0.4304,\n",
      "        0.3918, 0.3453, 0.1038, 0.2841, 0.3742, 0.4736, 0.4126, 0.4344, 0.3898,\n",
      "        0.3568, 0.4816, 0.3510, 0.3137, 0.2298, 0.3150, 0.2656, 0.3076, 0.3281,\n",
      "        0.1746, 0.2882, 0.3100, 0.2576, 0.2265, 0.2052, 0.3062, 0.1652, 0.3047,\n",
      "        0.4019, 0.2638, 0.2034, 0.1700, 0.1720, 0.2012, 0.2040, 0.1606, 0.4601,\n",
      "        0.5585, 0.3275, 0.3916, 0.3816, 0.4454, 0.3584, 0.3783, 0.3711, 0.3588,\n",
      "        0.4458, 0.4045, 0.4056, 0.2271, 0.1454, 0.2993, 0.1614, 0.4524, 0.4240,\n",
      "        0.5058, 0.3968, 0.4525, 0.4808, 0.4460, 0.4066, 0.3421, 0.2923, 0.3030,\n",
      "        0.0845, 0.1935, 0.3766, 0.4264, 0.2828, 0.1739, 0.2213, 0.2043, 0.2779,\n",
      "        0.3171, 0.2920, 0.3155, 0.3624, 0.4724, 0.2753, 0.1559, 0.1459, 0.3216,\n",
      "        0.2008, 0.3328, 0.0690, 0.3348, 0.2381, 0.2163, 0.5185, 0.4322, 0.0331,\n",
      "        0.2959, 0.4506, 0.3907, 0.2745, 0.4135, 0.2420, 0.4134, 0.2999, 0.2903,\n",
      "        0.3176, 0.2953, 0.3352, 0.3685, 0.3717, 0.4317, 0.4677, 0.4812, 0.4141,\n",
      "        0.3737, 0.4948, 0.3168, 0.4032, 0.4826, 0.3488, 0.2744, 0.2949, 0.3313,\n",
      "        0.3894, 0.3878, 0.4160, 0.2812, 0.3080, 0.2323, 0.3373, 0.3351, 0.3333,\n",
      "        0.3643, 0.3414, 0.4806, 0.3159, 0.2733, 0.3754, 0.4323, 0.3372, 0.3440,\n",
      "        0.3922, 0.4079, 0.3841, 0.2726, 0.2462, 0.3922, 0.3273, 0.4217, 0.3148,\n",
      "        0.3681, 0.4502, 0.3779, 0.2986, 0.3523, 0.4247, 0.3965, 0.2881, 0.3630])\n"
     ]
    }
   ],
   "source": [
    "query = \"The New York Mets are very good\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# 4. Compute cosine similarities\n",
    "cos_scores = util.cos_sim(query_embedding, embeddings)[0]  # shape: (3,)\n",
    "\n",
    "# 5. Find most similar\n",
    "most_similar_idx = cos_scores.argmax()\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Most similar: {all_chunks[most_similar_idx]}\")\n",
    "print(f\"Scores: {cos_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745e8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseball-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
